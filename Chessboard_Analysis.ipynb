{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8035cc73",
   "metadata": {
    "id": "8035cc73"
   },
   "source": [
    "# Chessboard Analysis Notebook\n",
    "\n",
    "This program processes an image of a chessboard to analyze its structure, detect individual squares, and classify them as black or white. By leveraging image processing techniques such as contour detection, perspective transformation, and intensity analysis, the program ensures accurate identification of the chessboard grid. Additionally, it calculates the accuracy of the detected black and white squares against expected values, providing insights into the reliability of the detection process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944962aa",
   "metadata": {},
   "source": [
    "This code aims to process an image of a chessboard to identify and count the number of black and white squares while calculating the accuracy of detection. The process involves several key steps:\n",
    "\n",
    "   - **Image Preprocessing:** The input image is converted to grayscale, and histogram equalization is applied to enhance contrast.\n",
    "   - **Thresholding and Contour Detection:** The image is binarized using thresholding, and contours are detected to isolate the chessboard.\n",
    "   - **Perspective Transformation:** The largest contour is used to approximate the chessboard's corners and perform a perspective transform to obtain a cropped, top-down view.\n",
    "   - **Square Analysis:** The cropped chessboard is divided into an 8x8 grid, and each square's intensity is analyzed to classify it as black or white.\n",
    "   - **Accuracy Calculation:** The detected black and white squares are compared to expected values, and the accuracy of the detection is computed.\n",
    "   \n",
    "This workflow ensures robust detection and verification of chessboard squares, even in challenging lighting conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365a056",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e92c",
   "metadata": {
    "id": "8c11e92c"
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e099f2",
   "metadata": {},
   "source": [
    "**`expected_black`** and **`expected_white`**: Represent the standard count of black and white squares (32 each) on a chessboard.\n",
    "\n",
    "#### **Function: `show_image`** \n",
    "A utility to display images in Jupyter Notebooks for debugging and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ba43c",
   "metadata": {
    "id": "554ba43c"
   },
   "outputs": [],
   "source": [
    "# Initialize the expected count of black and white squares on a chessboard\n",
    "expected_black = 32\n",
    "expected_white = 32\n",
    "\n",
    "# Function to display an image with a title in a Jupyter Notebook\n",
    "# - 'title': Title of the image to be displayed.\n",
    "# - 'image': The image array to display.\n",
    "# - 'cmap': Optional; a colormap for grayscale images.\n",
    "# The function uses matplotlib to render the image and removes axes for better visualization.\n",
    "def show_image(title, image, cmap=None):\n",
    "    plt.figure(figsize=(8, 6))  # Set the figure size\n",
    "    plt.title(title)  # Set the title\n",
    "    if cmap:  \n",
    "        plt.imshow(image, cmap=cmap)  # Display grayscale image\n",
    "    else:\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for display\n",
    "    plt.axis(\"off\")  # Hide axes for a cleaner display\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9939c7",
   "metadata": {},
   "source": [
    "# Define paths to the input images\n",
    "\n",
    "#### **Purpose** \n",
    "* Load two images from specified file paths for further analysis.\n",
    "\n",
    "#### **Key Steps** \n",
    "1. **Image Paths**: Define the file paths for the images (`image_path1` and `image_path2`). \n",
    "2. **Load Images**: Use `cv2.imread` to read the images into memory.\n",
    "3.  **Validation**: Check if the images are loaded correctly: \n",
    "    * Print an error message if any image fails to load. \n",
    "    * Confirm successful loading if both images are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d046a0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "66d046a0",
    "outputId": "3237829d-9ba7-40fc-ba8b-7163efa99a93"
   },
   "outputs": [],
   "source": [
    "# Define paths to the input images\n",
    "image_path1 = \"data/image4.jpg\"  # Path to the first image\n",
    "image_path2 = \"data/image6.jpg\"  # Path to the second image\n",
    "\n",
    "# Load the images using OpenCV\n",
    "image1 = cv2.imread(image_path1)  # Load the first image\n",
    "image2 = cv2.imread(image_path2)  # Load the second image\n",
    "\n",
    "# Verify if the images have been loaded successfully\n",
    "if image1 is None or image2 is None:\n",
    "    print(\"Error: One or more images could not be loaded.\")  # Error message if loading fails\n",
    "else:\n",
    "    print(\"Images loaded successfully!\")  # Success message if both images are loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98555d9",
   "metadata": {},
   "source": [
    "### To visually inspect the original images and ensure they are loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6cd48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "26d6cd48",
    "outputId": "9ec63798-320a-4395-f51c-61119c96b468"
   },
   "outputs": [],
   "source": [
    "# Debug: Display the original images\n",
    "show_image(\"Original Image 1\", image1)\n",
    "\n",
    "show_image(\"Original Image 2\", image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac774e",
   "metadata": {},
   "source": [
    "# Convert the original images to grayscale\n",
    "To convert the images to grayscale, simplifying further image processing by reducing the color channels.\n",
    "Grayscale images are often used in computer vision tasks as they contain only intensity values, making processing faster and less complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9cbe12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "0f9cbe12",
    "outputId": "4aef28d3-1c34-45b6-a23a-529a275a55d7"
   },
   "outputs": [],
   "source": [
    "# Convert the original images to grayscale\n",
    "# Grayscale conversion simplifies image processing by reducing color channels to a single intensity channel\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)  # Convert the first image to grayscale\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)  # Convert the second image to grayscale\n",
    "\n",
    "# Debug Step: Visualize the grayscale images\n",
    "# Helps confirm successful conversion to grayscale\n",
    "show_image(\"Grayscale Image 1\", gray1)  # Display the first grayscale image\n",
    "show_image(\"Grayscale Image 2\", gray2)  # Display the second grayscale image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95aa99e",
   "metadata": {},
   "source": [
    "# Apply CLAHE\n",
    "Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to enhance the contrast of the grayscale images. CLAHE improves the local contrast by equalizing histograms in small regions (tiles) of the image, which is particularly useful in images with varying lighting conditions.\n",
    "\n",
    "## But in our case\n",
    "When increasing the `clipLimit` in CLAHE (Contrast Limited Adaptive Histogram Equalization), the contrast enhancement becomes more aggressive. While this can improve local contrast in certain areas, it can also introduce excessive noise or exaggerate irregularities in the image. This effect can distort the appearance of objects, like a chessboard, making it difficult for image processing algorithms to identify the board correctly, especially when the image is rotated.\n",
    "\n",
    "### Why It Happens:\n",
    "High `clipLimit` Values: When the `clipLimit` is too high, small details in the image become exaggerated, which may lead to abnormal contrast in different regions. In rotated or transformed images, the board might become harder to detect due to these unwanted contrast changes, particularly in areas that should remain uniform (e.g., the squares of a chessboard).\n",
    "Board Detection: The algorithm relies on clear edges and uniform regions (the chessboard squares). Excessive contrast or noise can make these features harder to detect, especially when the image is rotated.\n",
    "\n",
    "### Solution:\n",
    "To avoid abnormal contrast that disturbs chessboard detection:\n",
    "\n",
    "Use a moderate `clipLimit` value, typically between 1.0 and 2.0, based on the image's lighting and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c239f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "id": "38c239f8",
    "outputId": "10facec6-e2d7-479b-e1ab-a66d24ed8d54"
   },
   "outputs": [],
   "source": [
    "# Apply CLAHE (Contrast Limited Adaptive Histogram Equalization) to improve the contrast of the grayscale images\n",
    "# CLAHE enhances local contrast by applying histogram equalization to small tiles, reducing over-amplification of noise\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Create CLAHE object with specified parameters\n",
    "\n",
    "# Apply CLAHE to the grayscale images\n",
    "gray_equalized1 = clahe.apply(gray1)  # Apply CLAHE to the first grayscale image\n",
    "gray_equalized2 = clahe.apply(gray2)  # Apply CLAHE to the second grayscale image\n",
    "\n",
    "# Debug Step: Visualize the contrast-enhanced images\n",
    "# This helps to see the improvement in contrast after CLAHE is applied\n",
    "show_image(\"Histogram Equalized Image 1\", gray_equalized1)  # Display the first CLAHE-enhanced image\n",
    "show_image(\"Histogram Equalized Image 2\", gray_equalized2)  # Display the second CLAHE-enhanced image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c302400",
   "metadata": {},
   "source": [
    "# Apply binary inverse thresholding \n",
    "To apply binary inverse thresholding, which converts the grayscale image into a binary image with inverted values. This step helps isolate the chessboard grid by blacking out the background and making the grid easily detectable for contour analysis.\n",
    "\n",
    "### Reason for Inverse Thresholding\n",
    "- In the input images, the chessboard grid has a surrounding edge border that helps identify the entire grid area.\n",
    "- By inverting the binary threshold:\n",
    "    - White (1) represents the surrounding edge border, as it has higher intensity values, making it easier to detect contours.\n",
    "    - Black (0) represents the background, effectively eliminating it from contour detection.\n",
    "- This ensures only the grid and its borders are considered for further processing, reducing noise and improving accuracy in finding the 8x8 grid.\n",
    "\n",
    "### Debug:\n",
    "Display the thresholded images to confirm that the background is blacked out, leaving the chessboard surrounding edge border."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38a8e7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "id": "6a38a8e7",
    "outputId": "ed78f87d-8024-4325-99f9-8c6e93b3be8f"
   },
   "outputs": [],
   "source": [
    "# Apply binary inverse thresholding to the equalized grayscale images\n",
    "# The threshold value of 127 is used to separate the foreground (white) from the background (black)\n",
    "# The cv2.THRESH_BINARY_INV flag inverts the thresholding result, where values greater than 127 become black (0), and values less than 127 become white (255)\n",
    "_, thresholded1 = cv2.threshold(gray_equalized1, 127, 255, cv2.THRESH_BINARY_INV)  # Apply thresholding to the first image\n",
    "_, thresholded2 = cv2.threshold(gray_equalized2, 127, 255, cv2.THRESH_BINARY_INV)  # Apply thresholding to the second image\n",
    "\n",
    "# Debug Step: Display the thresholded (binary inverse) images\n",
    "# This helps visualize the segmentation result after thresholding\n",
    "show_image(\"Thresholded Image 1\", thresholded1)  # Display the thresholded version of the first image\n",
    "show_image(\"Thresholded Image 2\", thresholded2)  # Display the thresholded version of the second image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca7aed4",
   "metadata": {},
   "source": [
    "# Find contours\n",
    "## What is a Contour?\n",
    "A contour is a curve or boundary that joins all continuous points with the same color or intensity in an image. In computer vision, contours are used to represent the shape and boundary of objects within an image. Contours are essentially edges but grouped into connected components for object representation.\n",
    "\n",
    "## Why Do We Use Contours?\n",
    "Contours help:\n",
    "\n",
    "- Detect Object Boundaries: Identify the outer shape or edges of objects in an image.\n",
    "- Feature Extraction: Analyze the structure and geometry of objects (e.g., area, perimeter, orientation).\n",
    "- Segmentation: Separate an object from its background.\n",
    "- Shape Analysis: Classify or compare objects based on their shapes.\n",
    "\n",
    "## Use of Contours in our Code\n",
    "\n",
    "1. Locate the Chessboard:\n",
    "\n",
    "    - The largest contour likely corresponds to the chessboard, as the thresholded image emphasizes its boundary.\n",
    "    - Sorting contours by area helps prioritize the grid's boundary over smaller, irrelevant objects.\n",
    "\n",
    "2. Filter Noise:\n",
    "\n",
    "    - Using binary inverse thresholding, the background is blacked out, leaving the chessboard in white. This ensures the contour-detection algorithm focuses on the chessboard instead of noise or unwanted details.\n",
    "\n",
    "3. Grid Detection:\n",
    "\n",
    "    - The chessboard grid is enclosed within the detected contour. Once the contour of the grid is isolated, further processing can refine the grid lines and detect the 8x8 squares.\n",
    "\n",
    "4. Visual Confirmation:\n",
    "\n",
    "    - Drawing contours over the image helps debug and confirm that the desired object (the chessboard) is detected correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9f888",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "id": "a1f9f888",
    "outputId": "f1e7418b-b8b7-4ba5-ea32-fd5121cbfd9a"
   },
   "outputs": [],
   "source": [
    "# Find contours in the thresholded images\n",
    "# cv2.RETR_EXTERNAL: Retrieves only the external contours (outermost boundaries).\n",
    "# cv2.CHAIN_APPROX_SIMPLE: Compresses horizontal, vertical, and diagonal segments, keeping only essential points to represent the contour.\n",
    "contours1, _ = cv2.findContours(thresholded1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # Find contours in the first image\n",
    "contours2, _ = cv2.findContours(thresholded2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # Find contours in the second image\n",
    "\n",
    "# Sort contours by area in descending order\n",
    "# This ensures the largest contour (likely the chessboard) is considered first\n",
    "contours1 = sorted(contours1, key=cv2.contourArea, reverse=True)  # Sort contours for the first image\n",
    "if contours1:  # Check if any contours are found\n",
    "    # Debug Step: Display contours on the first image\n",
    "    contour_debug1 = image1.copy()  # Create a copy of the original image for visualization\n",
    "    cv2.drawContours(contour_debug1, contours1, -1, (0, 0, 255), 2)  # Draw all contours in red with thickness 2\n",
    "    show_image(\"Contours Image 1\", contour_debug1)  # Display the contours overlaid on the first image\n",
    "else:\n",
    "    print(\"Error: No contours found for first image.\")  # Display an error message if no contours are detected\n",
    "\n",
    "contours2 = sorted(contours2, key=cv2.contourArea, reverse=True)  # Sort contours for the second image\n",
    "if contours2:  # Check if any contours are found\n",
    "    # Debug Step: Display contours on the second image\n",
    "    contour_debug2 = image2.copy()  # Create a copy of the original image for visualization\n",
    "    cv2.drawContours(contour_debug2, contours2, -1, (0, 0, 255), 2)  # Draw all contours in red with thickness 2\n",
    "    show_image(\"Contours Image 2\", contour_debug2)  # Display the contours overlaid on the second image\n",
    "else:\n",
    "    print(\"Error: No contours found for second image.\")  # Display an error message if no contours are detected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb7de4c",
   "metadata": {},
   "source": [
    "# Approximate the polygonal curve\n",
    "This step simplifies the largest contour (chessboard boundary) into a polygon with fewer points, retaining its shape while reducing unnecessary details.\n",
    "\n",
    "### Why This is Needed?\n",
    "- Simplifies the detected contour for easier processing.\n",
    "- Focuses on key geometric features, like the corners of the chessboard.\n",
    "- Reduces noise and helps in tasks such as aligning or identifying the grid structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-tl2u2jrh8SW",
   "metadata": {
    "id": "-tl2u2jrh8SW"
   },
   "outputs": [],
   "source": [
    "# Approximate the polygonal curve of the largest contour\n",
    "# cv2.arcLength() calculates the perimeter of the contour\n",
    "# epsilon defines the maximum distance between the original contour and its approximation; 2% of the perimeter is used here\n",
    "epsilon1 = 0.02 * cv2.arcLength(contours1[0], True)  # Calculate epsilon for the largest contour in the first image\n",
    "approx1 = cv2.approxPolyDP(contours1[0], epsilon1, True)  # Approximate the polygonal curve for the largest contour in the first image\n",
    "\n",
    "epsilon2 = 0.02 * cv2.arcLength(contours2[0], True)  # Calculate epsilon for the largest contour in the second image\n",
    "approx2 = cv2.approxPolyDP(contours2[0], epsilon2, True)  # Approximate the polygonal curve for the largest contour in the second image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36366a0e",
   "metadata": {},
   "source": [
    "# Visualize the approximated corners\n",
    "\n",
    "### Why Do This?\n",
    "- This step visually confirms that the corners of the approximated contour (chessboard boundary) are correctly identified and labeled.\n",
    "- It helps ensure the approximation and subsequent grid alignment are accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18254d0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 978
    },
    "id": "18254d0f",
    "outputId": "867c4733-7f50-4a8a-92fb-c2d61f0663a4"
   },
   "outputs": [],
   "source": [
    "# Visualize the approximated corners on the images\n",
    "# Combine the images and their corresponding approximations into lists for processing\n",
    "images = [image1, image2]  # List of original images\n",
    "approxs = [approx1, approx2]  # List of approximated polygonal curves (corners)\n",
    "\n",
    "# Iterate over each image and its approximated corners\n",
    "for idx, image in enumerate(images, start=1):  \n",
    "    debug_image = image.copy()  # Create a copy of the original image for visualization\n",
    "    for i, point in enumerate(approxs[idx-1]):  # Iterate through each point in the approximated polygon\n",
    "        x, y = point.ravel()  # Flatten the coordinates of the corner point\n",
    "        cv2.circle(debug_image, (x, y), 20, (0, 0, 255), -1)  # Draw a red circle at each corner\n",
    "        cv2.putText(debug_image, f\"{i + 1}\", (x + 10, y + 10), cv2.FONT_HERSHEY_SIMPLEX, 2.5, (255, 0, 0), 5)  \n",
    "        # Label each corner with its index number in blue text\n",
    "    show_image(\"Approximated Corners\", debug_image)  # Display the image with corners highlighted and labeled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4521643",
   "metadata": {},
   "source": [
    "# Chessboard Processing Function\n",
    "\n",
    "## Purpose\n",
    "The `process_chessboard` function evaluates the number of black and white squares on a chessboard image and calculates their detection accuracy compared to expected values.\n",
    "\n",
    "\n",
    "\n",
    "1. **Perspective Transformation**\n",
    "The chessboard is transformed into a top-down view using the detected corners. This ensures proper alignment, simplifying square-based analysis.\n",
    "\n",
    "2. **Square Division**\n",
    "The transformed image is divided into an 8x8 grid to correspond to the chessboard squares. Each square is processed individually to determine if it is black or white.\n",
    "\n",
    "3. **Square Classification**\n",
    "Each square is analyzed based on its intensity:\n",
    "   - The **mean intensity** of each square (`np.mean(square)`) is calculated. This represents the average brightness of the square.\n",
    "   - A **dynamic threshold** (`np.mean(gray_cropped)`) is used to classify the square as black or white. Squares darker than the threshold are classified as black, and lighter squares as white.\n",
    "\n",
    "\n",
    "### Why Use `np.mean(gray_cropped)`?\n",
    "\n",
    "The mean intensity of the entire grayscale chessboard image (`gray_cropped`) serves as a **dynamic threshold**. Here's why:\n",
    "\n",
    "1. **Dynamic Adaptability**: \n",
    "   - Different chessboards or lighting conditions can vary in overall brightness. Using the mean intensity of the whole image ensures the threshold adapts to these variations.\n",
    "   - For example, if the image is dimly lit, the mean intensity will be lower, resulting in a lower threshold. This prevents misclassification of squares.\n",
    "\n",
    "2. **Avoiding Fixed Thresholds**:\n",
    "   - A fixed intensity value might not work well across varying images because of differences in lighting, shadows, or camera exposure. \n",
    "   - The dynamic approach ensures the method remains robust across a variety of conditions.\n",
    "\n",
    "3. **Global Context**:\n",
    "   - By considering the mean intensity of the entire chessboard, the method establishes a baseline to distinguish between darker (black) and lighter (white) squares relative to the overall brightness.\n",
    "\n",
    "\n",
    "### Challenges with `np.mean(gray_cropped)`\n",
    "While effective, using the mean intensity of the entire image can encounter issues:\n",
    "- **Chess Pieces**: The presence of chess pieces can alter the mean intensity of individual squares, leading to potential misclassifications.\n",
    "- **Complex Patterns**: Boards with significant texture or design might affect the accuracy of this approach.\n",
    "\n",
    "\n",
    "### Why Use This Approach?\n",
    "\n",
    "1. **Simplicity**: Calculating mean intensities is computationally efficient and straightforward.\n",
    "2. **Robustness**: The dynamic threshold adapts to lighting and image conditions, making the method versatile.\n",
    "3. **Visual Feedback**: The color-coded visualization helps debug and refine the classification process.\n",
    "\n",
    "### Improvements to Consider\n",
    "- **Advanced Techniques**: machine learning models could improve square classification, especially in challenging scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f8dfd",
   "metadata": {
    "id": "b77f8dfd"
   },
   "outputs": [],
   "source": [
    "def process_chessboard(image, approx, expected_black, expected_white, debug=True):\n",
    "    \"\"\"\n",
    "    Processes a chessboard image to evaluate black and white squares.\n",
    "\n",
    "    Args:\n",
    "        image: Input chessboard image.\n",
    "        approx: Approximated corners of the chessboard contour.\n",
    "        expected_black: Expected count of black squares (typically 32).\n",
    "        expected_white: Expected count of white squares (typically 32).\n",
    "        debug: Flag to enable/disable debug visualizations.\n",
    "\n",
    "    Returns:\n",
    "        black_accuracy: Accuracy of black square detection.\n",
    "        white_accuracy: Accuracy of white square detection.\n",
    "        overall_accuracy: Combined accuracy of both black and white square detection.\n",
    "    \"\"\"\n",
    "    # Ensure at least 4 points are available to define the chessboard corners\n",
    "    if len(approx) >= 4:\n",
    "        # Extract the first 4 corners and convert them into a float32 array\n",
    "        points = np.array([point.ravel() for point in approx[:4]], dtype=\"float32\")\n",
    "\n",
    "        # Calculate the width and height of the transformed chessboard\n",
    "        width = int(max(np.linalg.norm(points[0] - points[1]), np.linalg.norm(points[2] - points[3])))\n",
    "        height = int(max(np.linalg.norm(points[0] - points[3]), np.linalg.norm(points[1] - points[2])))\n",
    "\n",
    "        # Define the destination rectangle for perspective transformation\n",
    "        dst_rect = np.array([[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]], dtype=\"float32\")\n",
    "\n",
    "        # Compute the perspective transform matrix\n",
    "        M = cv2.getPerspectiveTransform(points, dst_rect)\n",
    "\n",
    "        # Apply the perspective transformation to obtain a top-down view of the chessboard\n",
    "        cropped_image = cv2.warpPerspective(image, M, (width, height))\n",
    "\n",
    "        # Debug: Display the cropped chessboard image\n",
    "        if debug:\n",
    "            show_image(\"Cropped Chessboard\", cropped_image)\n",
    "\n",
    "        # Divide the chessboard into 8x8 squares\n",
    "        square_size = (cropped_image.shape[1] // 8, cropped_image.shape[0] // 8)\n",
    "        black_count = 0  # Count of detected black squares\n",
    "        white_count = 0  # Count of detected white squares\n",
    "\n",
    "        # Convert the cropped image to grayscale for intensity analysis\n",
    "        gray_cropped = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2GRAY)\n",
    "        color_coded = cropped_image.copy()  # Copy of the image for color-coded visualization\n",
    "\n",
    "        # Loop through each square in the 8x8 grid\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                # Determine the bounding box for the current square\n",
    "                x_start = col * square_size[0]\n",
    "                y_start = row * square_size[1]\n",
    "                x_end = x_start + square_size[0]\n",
    "                y_end = y_start + square_size[1]\n",
    "\n",
    "                # Extract the square's region from the grayscale image\n",
    "                square = gray_cropped[y_start:y_end, x_start:x_end]\n",
    "                ##show_image(\"cropped_image\", square)\n",
    "                mean_intensity = np.mean(square)  # Calculate the mean intensity of the square\n",
    "                dynamic_threshold = np.mean(gray_cropped)  # Calculate a dynamic threshold based on the whole image (on some image getting value near 144)\n",
    "                # Classify the square as black or white based on its intensity\n",
    "                if mean_intensity < dynamic_threshold:  # Square is darker than the threshold\n",
    "                    black_count += 1\n",
    "                    color = (0, 0, 0)  # Black for visualization\n",
    "                else:  # Square is lighter than the threshold\n",
    "                    white_count += 1\n",
    "                    color = (255, 255, 255)  # White for visualization\n",
    "\n",
    "                # Fill the square with the detected color on the visualization image\n",
    "                cv2.rectangle(color_coded, (x_start, y_start), (x_end, y_end), color, -1)\n",
    "\n",
    "        # Debug: Display the color-coded chessboard visualization\n",
    "        if debug:\n",
    "            show_image(\"Color-coded Chessboard\", color_coded)\n",
    "\n",
    "        # Print the detected counts of black and white squares\n",
    "        print(f\"Black squares: {black_count}\")\n",
    "        print(f\"White squares: {white_count}\")\n",
    "\n",
    "        # Adjust counts to fit the expected values if necessary\n",
    "        if black_count > expected_black:\n",
    "            white_count = 64 - black_count  # Recalculate white count to maintain the 8x8 total\n",
    "\n",
    "        # Calculate the number of correctly detected black and white squares\n",
    "        correct_black = min(black_count, expected_black)\n",
    "        correct_white = min(white_count, expected_white)\n",
    "\n",
    "        # Compute detection accuracy percentages\n",
    "        black_accuracy = (correct_black / expected_black) * 100\n",
    "        white_accuracy = (correct_white / expected_white) * 100\n",
    "        overall_accuracy = ((correct_black + correct_white) / (expected_black + expected_white)) * 100\n",
    "\n",
    "        # Print the accuracy results\n",
    "        print(f\"Black Square Accuracy: {black_accuracy:.2f}%\")\n",
    "        print(f\"White Square Accuracy: {white_accuracy:.2f}%\")\n",
    "        print(f\"Overall Accuracy: {overall_accuracy:.2f}%\")\n",
    "\n",
    "        # Return the accuracy values\n",
    "        return black_accuracy, white_accuracy, overall_accuracy\n",
    "    else:\n",
    "        # Error handling if fewer than 4 corners are detected\n",
    "        print(\"Error: Could not detect four corners.\")\n",
    "        return 0, 0, 0  # Return zero accuracy in case of failure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-XiL2vQCkBro",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-XiL2vQCkBro",
    "outputId": "25474158-a288-4b7f-d1eb-5b4435f1fca5"
   },
   "outputs": [],
   "source": [
    "# Process first image\n",
    "black_acc1, white_acc1, overall_acc1 = process_chessboard(\n",
    "    image1, approx1, expected_black, expected_white, debug=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ElOog9mkMgL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6ElOog9mkMgL",
    "outputId": "c7ba0657-5092-428a-aa89-5aa6b50a7c0d"
   },
   "outputs": [],
   "source": [
    "# Process second image\n",
    "black_acc2, white_acc2, overall_acc2 = process_chessboard(\n",
    "    image2, approx2, expected_black, expected_white, debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
